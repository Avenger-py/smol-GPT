# smol-GPT
Built a small (22M params) GPT 2 like model from scratch and trained it on Wikitext103 dataset.
