# smol-GPT
Built a small (22M params) GPT 2 like transformer language model from scratch and trained it on Wikitext103 dataset.

## Results

## Dependencies
```
pip install torch numpy tiktoken wandb tqdm sklearn pandas
```
## Dataset
## Training
## Inference
## Evaluation
